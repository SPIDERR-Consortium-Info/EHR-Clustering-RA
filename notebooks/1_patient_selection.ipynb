{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Patient Selection\n",
    "Summary:\n",
    "1. In this script we elucidate the first consult for each patient in HIX\n",
    "2. We remove patients that were admitted before 2011 (because we cannot guarantee that we have all the data avail)\n",
    "3. We export / save different patient selections to be used in the proceeding research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PATH_SCRIPTS= r'../src/scripts'\n",
    "PATH_MODULES= r'/exports/reum/tdmaarseveen/modules/' # unused\n",
    "PATH_FILES = r\"../../EMR_mining/output_files/\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(PATH_SCRIPTS)\n",
    "sys.path.append(r'../src/1_emr_scripts')\n",
    "#sys.path.append(PATH_MODULES)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Preload function\n",
    "def getFirstPEC(pat):\n",
    "    if pat in clean_dict.keys():\n",
    "        return clean_dict[int(pat)]\n",
    "    else :\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #1 Import DDR_A\n",
    "We will leverage the conclusion section of the DDR_A table (HIX) for our prediction of the RA diagnosis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #1.1 Import from latest HIX extraction (PUB)\n",
    "- keep in mind patient ids are pseudonymized in this version\n",
    "- We chose to not use the latest HiX patients because we did not have ethical consent for these patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tdmaarseveen/.conda/envs/ra_clustering2/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3145: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_REUTOT = pd.read_csv(r'../data/1_raw/Clustering_Conclusion_Consults.csv', sep=\";\", header = None)\n",
    "df_REUTOT.columns = ['id', 'STELLING', 'XANTWOORD', 'DATUM', 'PATNR', 'DATUM_A', 'Author']\n",
    "\n",
    "# Key file to link pseudonymized keys to pat ids\n",
    "df_pat = pd.read_csv(r'../data/1_raw/Pseudonymized_Keys.csv', sep=';')\n",
    "\n",
    "# Create dictionary from key file\n",
    "di = dict(zip(df_pat.member_entity_Pseudonym_value, df_pat.member_entity_Patient_value))\n",
    "def unblind(x):\n",
    "    try:\n",
    "        return di[x]\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "# Unblind\n",
    "df_REUTOT['PATNR'] = df_REUTOT['PATNR'].astype(int).apply(lambda x: unblind(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #1.2 Import data freeze (MI_uitgifte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tdmaarseveen/.conda/envs/ra_clustering2/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3145: DtypeWarning: Columns (1,10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25993"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_REUTOT = pd.read_csv(PATH_FILES + r'DF_REU_tot.csv', sep=\"|\")\n",
    "df_REUTOT['DATUM_A'] = pd.to_datetime(df_REUTOT['DATUM_A'], format='%Y-%m-%d', errors='ignore') # DATUM_A is most reliable\n",
    "df_REUTOT['DATUM'] = pd.to_datetime(df_REUTOT['DATUM'], format='%Y-%m-%d', errors='ignore') # DATUM_A is most reliable\n",
    "len(df_REUTOT[df_REUTOT['STELLING']=='Type contact/bespreking']['PATNR'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1251\n"
     ]
    }
   ],
   "source": [
    "df_pat = pd.read_csv(r'../filters/RA_patients_083_new.csv', sep=',', index_col=0)\n",
    "new_pat = dict(zip(df_pat.PATNR, df_pat.NEW_PEC))\n",
    "print(len(new_pat.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #2 Acquire first consult (PEC)\n",
    "\n",
    "First, we select patients where first consult is registered (either PVC or PEC). The PEC has to be after 29 of August 2011, otherwise we can't guarantee that we have a complete set of records (HIX became obligatory in 2011) \n",
    "\n",
    "Next, we impute the PEC if we can't find a first consult :\n",
    "- We postulate that consults that aren't preceded by other consults in over a year most likely concern a first consult (PEC)!\n",
    "- Consequently we had to employ a cut-off before imputing the PEC. To ensure that we have access to the consults in the preceding year we needed to guarantee that the information was registered in HiX. We applied a threshold, only requesting patients after August 2012 (since HiX was only initialized a mere year ago: August 2011)\n",
    "- Furthermore, we removed the entries after 16-07-2020 (because this data freeze has entries up to 06-04-2019)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25993\n",
      "yeah 10758   1900-01-01 00:00:00+00:00\n",
      "10757   1900-01-01 00:00:00+00:00\n",
      "10756   1900-01-01 00:00:00+00:00\n",
      "Name: DATUM, dtype: datetime64[ns, UTC] old 1900-01-01 00:00:00+00:00\n",
      "yeah 10758   1900-01-01 00:00:00+00:00\n",
      "10757   1900-01-01 00:00:00+00:00\n",
      "10756   1900-01-01 00:00:00+00:00\n",
      "Name: DATUM, dtype: datetime64[ns, UTC] old 1900-01-01 00:00:00+00:00\n",
      "yeah 10758   1900-01-01 00:00:00+00:00\n",
      "10757   1900-01-01 00:00:00+00:00\n",
      "10756   1900-01-01 00:00:00+00:00\n",
      "Name: DATUM, dtype: datetime64[ns, UTC] old 1900-01-01 00:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "from math import isnan\n",
    "import numpy as np\n",
    "\n",
    "l_patients = {}\n",
    "\n",
    "# create subset with consult types\n",
    "sub_df = df_REUTOT[df_REUTOT['STELLING']=='Type contact/bespreking'].copy() # can we link first date to consult type?\n",
    "print(len(sub_df['PATNR'].unique()))\n",
    "\n",
    "sub_df2 = sub_df[sub_df['XANTWOORD'].isin(['pec', 'pvc'])].copy() # , 'pvc'\n",
    "sub_df2 = sub_df2.sort_values('DATUM').drop_duplicates('PATNR',keep='first') # keep 'first' first consult\n",
    "d_pec =sub_df2.groupby(['PATNR'])['DATUM'].agg('min').to_dict()\n",
    "\n",
    "## Impute PEC\n",
    "d_first_date =sub_df.groupby(['PATNR'])['DATUM'].agg('min').to_dict()\n",
    "\n",
    "\n",
    "for pat in d_first_date.keys():\n",
    "    if pat in d_pec.keys(): # IF PEC\n",
    "        if d_pec[pat].tz_localize(None)==pd.to_datetime('19000101', format='%Y%m%d', errors='ignore'):\n",
    "            \n",
    "            d_first_date[pat] = sub_df2[pd.DatetimeIndex(sub_df2['DATUM']).tz_localize(None)==pd.to_datetime('19000101', format='%Y%m%d', errors='ignore')]['DATUM']\n",
    "            print('yeah', d_first_date[pat], 'old', d_pec[pat])\n",
    "        else :\n",
    "            d_first_date[pat] = d_pec[pat]\n",
    "        if d_pec[pat].tz_localize(None) > pd.to_datetime('20180617', format='%Y%m%d', errors='ignore'):\n",
    "            d_first_date[pat] = np.nan\n",
    "        # First allowed data is 29-07-2011\n",
    "        if d_pec[pat].tz_localize(None) < pd.to_datetime('20110729', format='%Y%m%d', errors='ignore'): # smaller than init HIX\n",
    "            d_first_date[pat] = np.nan\n",
    "    else : # IF ANYTHING BUT PEC -> TRY TO IMPUTE\n",
    "        date = d_first_date[pat]\n",
    "        consult = sub_df[((sub_df['PATNR']==pat) & (pd.to_datetime(sub_df['DATUM'], format='%Y%m%d', errors='ignore')==date))]['XANTWOORD'].iloc[0]\n",
    "        if consult != 'PEC':\n",
    "            # we know that it is the first consult -> hence check if it is one year after creation of HIX\n",
    "            if  date.tz_localize(None) < pd.to_datetime('20120729', format='%Y%m%d', errors='ignore'):\n",
    "                d_first_date[pat] = np.nan\n",
    "        # Latest allowed data is 17-06-2018\n",
    "        if date.tz_localize(None) > pd.to_datetime('20180617', format='%Y%m%d', errors='ignore'):\n",
    "            d_first_date[pat] = np.nan\n",
    "        \n",
    "\n",
    "# Filter out the patients without a (imputed) PEC \n",
    "clean_dict = {k: d_first_date[k] for k in d_first_date if type(d_first_date[k])!=float}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #2.1 Export patient with first date to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23180"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt =0\n",
    "with open('../filters/RA_imputed_NEW.dict', 'w') as f: \n",
    "    f.write('{')\n",
    "    for key, value in clean_dict.items():\n",
    "        #if value in ['pvc', 'pec']: # later maybe also include the possibilities aside from pvc??\n",
    "        if cnt != len(clean_dict)-1:\n",
    "            f.write('%s : \"%s\",' % (key, clean_dict[key]))\n",
    "        else :\n",
    "            f.write('%s : \"%s\"' % (key, clean_dict[key]))\n",
    "        cnt += 1\n",
    "    f.write('}')\n",
    "cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #3 Apply machine learning model to identify RA patients\n",
    "- select conclusion section\n",
    "- format date\n",
    "- assess date of first consult\n",
    "- apply cut-off (only include entries after 2011 (initialization of HIX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preload capture function that converts the first consult date from type string to type date\n",
    "def capture(row):\n",
    "    x = row['PEC']\n",
    "    try:\n",
    "        return pd.to_datetime(str(x))\n",
    "    except:\n",
    "        if 10757 in x: # weird artefact???\n",
    "            return pd.to_datetime('2017-12-06 00:00:00+00:00', format='%Y-%m-%d', errors='ignore')\n",
    "        print(row, 'Not Captured')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137466\n",
      "43038\n"
     ]
    }
   ],
   "source": [
    "import Preprocessing as func\n",
    "\n",
    "df_REUTOT = df_REUTOT[df_REUTOT['STELLING']=='Conclusie']\n",
    "print(len(df_REUTOT))\n",
    "df_REUTOT['PEC'] = df_REUTOT['PATNR'].apply(lambda x: getFirstPEC(x)) # apply first date\n",
    "df_REUTOT['XANTWOORD'] = df_REUTOT['XANTWOORD'].apply(lambda x : func.processArtefactsXML(str(x)))\n",
    "df_REUTOT = df_REUTOT[(~df_REUTOT['PEC'].isna())]\n",
    "df_REUTOT['PEC'] = df_REUTOT.apply(lambda x: capture(x), axis=1)\n",
    "df_REUTOT['DATUM'] =df_REUTOT['DATUM'].apply(lambda x: pd.to_datetime(x, utc=True, format='%Y-%m-%d', errors='ignore'))\n",
    "df_REUCON = df_REUTOT[((pd.DatetimeIndex(df_REUTOT['DATUM']).tz_localize(None) < pd.DatetimeIndex(df_REUTOT['PEC']).tz_localize(None) + pd.DateOffset(years=1, months=0, days=1)) & (pd.DatetimeIndex(df_REUTOT['DATUM']).tz_localize(None) >= pd.DatetimeIndex(df_REUTOT['PEC']).tz_localize(None)))]\n",
    "df_REUCON = df_REUCON[((pd.DatetimeIndex(df_REUCON['DATUM']).tz_localize(None) > pd.to_datetime('20110829', format='%Y%m%d', errors='ignore')) & (pd.DatetimeIndex(df_REUCON['PEC']).tz_localize(None) > pd.to_datetime('20110829', format='%Y%m%d', errors='ignore')))]\n",
    "print(len(df_REUCON))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #3.1 Preprocessing pipeline\n",
    "\n",
    "- Cast entry based table to a patient based format.\n",
    "- Perform lemmatization & remove textual artefacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import NLP_functions as func\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "#model_name='savedModels/SVM.sav'\n",
    "X_column='XANTWOORD'\n",
    "y_column=\"\"#'Outcome'\n",
    "id_column='PATNR'\n",
    "\n",
    "\n",
    "def entriesPatientMerge(pat_df, id_column, X_column, y_column=\"\"):\n",
    "    \"\"\"\n",
    "    Merges the entries into one entry per patient (according to the id_column)\n",
    "    \n",
    "    Input: \n",
    "        id_column = column with patient id\n",
    "        X_column = column with \n",
    "    \"\"\"\n",
    "    field = ''\n",
    "    for i in pat_df[X_column]:\n",
    "        field += \" \" + i + \" \"\n",
    "    if y_column!=\"\":\n",
    "        return {X_column: field, id_column : pat_df[id_column].iloc[0], y_column : pat_df[y_column].iloc[0]}\n",
    "    else :\n",
    "        return {X_column: field, id_column : pat_df[id_column].iloc[0]}\n",
    "\n",
    "if y_column != \"\":\n",
    "    df_ult = pd.DataFrame(columns=[X_column,  id_column, y_column])\n",
    "else : \n",
    "    df_ult = pd.DataFrame(columns=[X_column,  id_column])\n",
    "\n",
    "for pat in df_REUCON[id_column].unique():\n",
    "    print(pat)\n",
    "    pat_df = df_REUCON[df_REUCON[id_column]==pat]\n",
    "    if y_column != \"\":\n",
    "        df_ult = df_ult.append(entriesPatientMerge(pat_df, id_column, X_column, y_column), ignore_index=True)\n",
    "    else : \n",
    "        df_ult = df_ult.append(entriesPatientMerge(pat_df, id_column, X_column), ignore_index=True)\n",
    "\n",
    "df_ult['XANTWOORD'] = df_ult['XANTWOORD'].apply(lambda x : func.lemmatizingText(x, lan='nl')) # 'en' for english & 'de' for german\n",
    "df_ult=df_ult.fillna('')\n",
    "df_ult['XANTWOORD'] = df_ult['XANTWOORD'].apply(lambda x : func.processArtefactsXML(str(x)))\n",
    "df_ult.to_csv(r'../data/1_raw/processed_conclusions_date_2.csv', sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #3.2 Apply SVM\n",
    "- import pickled machine learning model\n",
    "- write predictions to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for Making predictions (n=17315) : 9.187673807144165\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import time\n",
    "import sklearn\n",
    "# get EMR text\n",
    "text_column = 'XANTWOORD'\n",
    "X = df_ult[text_column].values\n",
    "\n",
    "# apply built model on provided text\n",
    "loaded_model = pickle.load(open(r'../models/SVM_29042020.sav', 'rb')) # SVM_29042020.sav #  SVM_22012020.sav\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "probas_ = loaded_model.predict_proba(X)\n",
    "pred = probas_[:,1]\n",
    "\n",
    "t1 = time.time()\n",
    "print('Time for Making predictions (n=' + str(len(df_ult['PATNR'].unique())) + ') : ' + str(t1-t0))\n",
    "\n",
    "# add predictions to table\n",
    "df_ult['prediction'] = df_ult[text_column].copy()\n",
    "df_ult['prediction'] = pred\n",
    "\n",
    "df_ult[['PATNR', 'prediction']].to_csv(r'../data/1_raw/RA_patients_pred_3.csv', sep='|', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #4 Export patient consult information\n",
    "\n",
    "Write down important patient info in a single table:\n",
    "1. Patient id\n",
    "2. Probability\n",
    "3. Date of first consult (or imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load predictions\n",
    "df_ult = pd.read_csv(r'../data/1_raw/RA_patients_pred_3.csv', sep='|')\n",
    "\n",
    "# Add first consult date\n",
    "df_ult['PEC'] = df_ult['PATNR'].apply(lambda x: getFirstPEC(x))\n",
    "\n",
    "## Export \n",
    "df_ult.to_csv('../data/1_raw/RA_patients_pred_pec.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #4.1 Update PEC 16-07-2021\n",
    "- we discovered that the PEC was sometimes registered after the PVC (follow-up consult), thus we updated the function to count PVC as PEC if it was mentioned first. We updated our consult information accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pat = pd.read_csv('../data/1_raw/RA_patients_pred_pec.csv',sep=',')\n",
    "df_pat['NEW_PEC'] = df_pat['PATNR'].apply(lambda x: getFirstPEC(x))\n",
    "subset_df = df_pat[((df_pat['prediction']>=0.83) & (~df_pat['PEC'].isna()))].copy()\n",
    "subset_df = subset_df[(pd.DatetimeIndex(pd.to_datetime(subset_df['PEC'], format='%Y%m%d', errors='ignore')).tz_localize(None) > pd.to_datetime('20110829', format='%Y%m%d', errors='ignore'))]\n",
    "subset_df[['PATNR', 'prediction', 'PEC', 'NEW_PEC']].to_csv('../filters/RA_patients_083_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>PEC</th>\n",
       "      <th>NEW_PEC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2012-01-31 00:00:00</td>\n",
       "      <td>2012-01-31 00:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2017-04-03 00:00:00</td>\n",
       "      <td>2011-06-22 00:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2012-03-26 00:00:00</td>\n",
       "      <td>2012-03-26 00:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2012-01-11 00:00:00</td>\n",
       "      <td>2012-01-11 00:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.973550</td>\n",
       "      <td>2016-02-19 00:00:00</td>\n",
       "      <td>2012-04-10 00:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22424</th>\n",
       "      <td>0.995983</td>\n",
       "      <td>2018-03-06 00:00:00</td>\n",
       "      <td>2018-03-06 00:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22471</th>\n",
       "      <td>0.997036</td>\n",
       "      <td>2017-11-21 00:00:00</td>\n",
       "      <td>2017-11-21 00:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22571</th>\n",
       "      <td>0.990080</td>\n",
       "      <td>2018-04-04 00:00:00</td>\n",
       "      <td>2018-04-04 00:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22648</th>\n",
       "      <td>0.988933</td>\n",
       "      <td>2018-04-06 00:00:00</td>\n",
       "      <td>2018-04-06 00:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23261</th>\n",
       "      <td>0.962010</td>\n",
       "      <td>2017-02-02 00:00:00</td>\n",
       "      <td>2017-02-02 00:00:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1251 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       prediction                  PEC                    NEW_PEC\n",
       "0        1.000000  2012-01-31 00:00:00  2012-01-31 00:00:00+00:00\n",
       "3        1.000000  2017-04-03 00:00:00  2011-06-22 00:00:00+00:00\n",
       "7        1.000000  2012-03-26 00:00:00  2012-03-26 00:00:00+00:00\n",
       "9        1.000000  2012-01-11 00:00:00  2012-01-11 00:00:00+00:00\n",
       "10       0.973550  2016-02-19 00:00:00  2012-04-10 00:00:00+00:00\n",
       "...           ...                  ...                        ...\n",
       "22424    0.995983  2018-03-06 00:00:00  2018-03-06 00:00:00+00:00\n",
       "22471    0.997036  2017-11-21 00:00:00  2017-11-21 00:00:00+00:00\n",
       "22571    0.990080  2018-04-04 00:00:00  2018-04-04 00:00:00+00:00\n",
       "22648    0.988933  2018-04-06 00:00:00  2018-04-06 00:00:00+00:00\n",
       "23261    0.962010  2017-02-02 00:00:00  2017-02-02 00:00:00+00:00\n",
       "\n",
       "[1251 rows x 3 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_df[['prediction', 'PEC', 'NEW_PEC']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clustering_ra",
   "language": "python",
   "name": "clustering_ra"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
