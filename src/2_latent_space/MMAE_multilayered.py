from keras import backend as K
#from tensorflow.keras.callbacks import EarlyStopping
from mmae.multimodal_autoencoder import MultimodalAutoencoder # MMAE
#import maui
#import maui.utils
import time
import sys
import ast 
import pandas as pd
#import phenograph
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import numpy as np
from sklearn.metrics.cluster import adjusted_rand_score
import matplotlib.pyplot as plt
import seaborn as sns # ensure it is installed in custom env
import bokeh.plotting as bp
from bokeh.models import HoverTool, BoxSelectTool, CustomJS, DataTable, ColumnDataSource, Tabs, TableColumn
from bokeh.models.widgets import Panel
from bokeh.plotting import figure, show,  output_notebook
from bokeh.layouts import row, column, gridplot
from bokeh.transform import factor_cmap
from bokeh.models import  CategoricalColorMapper, LinearColorMapper, Select,  Slider
from bokeh.io import output_file, show
import bokeh.io as bio
from matplotlib import pyplot
import tensorflowjs as tjs
import tensorflow as tf
from keras.models import model_from_json

# Set seed
#tf.keras.utils.set_random_seed(1234)
tf.random.set_seed(20221212)

EMBEDDING_SPACE = 'MMAE_embedding' 

# If there already exists a model then we load this prev. trained model by default (Apparently there is some variance that we cant control with a seed?)
CREATE_MODEL = True

# Define list of categorical columns
l_cat = ['RF', 'aCCPs', 'Sex']
# Add all OHE joints to the mix
df_man_cat = pd.read_csv('/exports/reum/tdmaarseveen/RA_Clustering/new_data/4_processed/DF_Mannequin_NEW_Engineered.csv', sep='|')
l_keep = [x for x in list(df_man_cat.columns) if x not in ['patnr', 'FirstConsult', 'pseudoId']]
l_cat.extend(l_keep)


def visualize_umap_bokeh_na(embedding, df, l_cat, l_binary=[], patient_id='patnr', cluster_id='PhenoGraph_clusters', title='', path=None):
    """
    This function generates a bokeh scatter plot based on the provided embedding 
    (which is generated by a dimension reduction technique)
    
    Input:
        embedding= dataframe / 2d array with distances
        df = dataframe with columns (in case dist only features the distances)
        l_cat = specify columns to showcase
        l_binary = indicates the binary columns where the prevalence should be calculated instead of the mean!
        patient_id = str indicating column of patient
        cluster_id = str indicating column of clusters
        title = title of the bokeh plot
        path = str indicating the path where to save the file
    
    Output:
        Interactive HTML with a UMAP render
    """
    cluster_ix = 0
    
    print(len(embedding), len(df))
    print(len([i for i in l_cat if (i not in df.columns)]), [i for i in l_cat if (i not in df.columns)])

    first_col = df[l_cat[0]] # c_first
    l_hex = ['#f00', '#fe0500', '#fc0a00', '#fb0f00', '#fa1400', '#f91900', '#f71e00', '#f62200', '#f52700', '#f42c00', '#f23000', '#f13500', '#f03a00', '#ee3e00', '#ed4200', '#ec4700', '#eb4b00', '#e94f00', '#e85400', '#e75800', '#e65c00', '#e46000', '#e36400', '#e26800', '#e16c00', '#df7000', '#de7300', '#d70', '#db7b00', '#da7f00', '#d98200', '#d88600', '#d68900', '#d58d00', '#d49000', '#d39300', '#d19700', '#d09a00', '#cf9d00', '#cda000', '#cca300', '#cba600', '#caa900', '#c8ac00', '#c7af00', '#c6b200', '#c5b500', '#c3b800', '#c2ba00', '#c1bd00', '#bfbf00', '#babe00', '#b5bd00', '#b0bc00', '#acba00', '#a7b900', '#a2b800', '#9db700', '#98b500', '#94b400', '#8fb300', '#8ab200', '#86b000', '#81af00', '#7dae00', '#79ac00', '#74ab00', '#70aa00', '#6ca900', '#68a700', '#64a600', '#60a500', '#5ca400', '#58a200', '#54a100', '#50a000', '#4c9e00', '#489d00', '#459c00', '#419b00', '#3d9900', '#3a9800', '#369700', '#339600', '#2f9400', '#2c9300', '#299200', '#269100', '#228f00', '#1f8e00', '#1c8d00', '#198b00', '#168a00', '#138900', '#108800', '#0d8600', '#0b8500', '#088400', '#058300', '#038100', '#008000']
    l_grey = '#bcbcbc'

    
    # Create shadow dataframe, because you don't want to take the nan as the max
    sub_df = df[~df[l_cat[0]].isna()].copy()
    c_first = [l_hex[round(i/max(sub_df[l_cat[0]]) * 100)] if i==i else l_grey for i in df[l_cat[0]] ]

    c_alpha = [1 if i == cluster_ix else 0.1 for i in df[cluster_id] ]
    
    # putting everything in a dataframe
    umap_df = pd.DataFrame(embedding, columns=['x', 'y'])
    
    d_col = dict(x=umap_df['x'], y=umap_df['y'], pt=df[patient_id], clust=df[cluster_id], c_col=c_first, c_alpha=c_alpha)
    
    # Add cluster column in case user didn't specify it in l_cat
    if cluster_id not in l_cat:
        l_cat.append(cluster_id)
    
    for cat in l_cat:
        d_col[cat] = df[cat]
    
    s1 = ColumnDataSource(data=d_col)
    #print(s1)
    #print(eql)
    
    umap_df['pt'] = df[patient_id]
    umap_df['value'] = first_col
    
    p3 = figure(plot_width=600, plot_height=500, tools="pan,wheel_zoom,box_zoom,reset,hover,save", title="An UMAP projection of %s patients" % (len(first_col)))
    cir = p3.circle('x', 'y', source=s1, alpha='c_alpha', line_color='c_col',  fill_color='c_col')
    
    
    
    # Create barplot to show prevalence of Feature 
    # Calculate prevalence of first feature
    prev = [round(len(df[((df[cluster_id]==i) & (df[l_cat[0]]==1))])/len(df[(df[cluster_id]==i)]), 3) for i in range(len(df[cluster_id].unique()))]
    
    s3 = ColumnDataSource(dict(clust=[i for i in range(len(df[cluster_id].unique()))],prev=prev))
    
    
    # Bar plot
    p4 = figure(plot_width=600, plot_height=500, tools="pan,wheel_zoom,box_zoom,reset,hover", title="Prevalence plot for categorical features")
    bar = p4.vbar('clust', top='prev', source=s3)
    
    hover2 = p4.select(dict(type=HoverTool)) # or p1
    hover2.tooltips={"clust": "@clust","prev" : "@prev"}
    
    # Violin plot
    q1= [np.percentile(df[(df[cluster_id]==i)][l_cat[0]].values, 25) for i in range(len(df[cluster_id].unique()))]
    q2= [np.percentile(df[(df[cluster_id]==i)][l_cat[0]].values, 50) for i in range(len(df[cluster_id].unique()))]
    q3= [np.percentile(df[(df[cluster_id]==i)][l_cat[0]].values, 75) for i in range(len(df[cluster_id].unique()))]
    iqr = np.array(q3) - np.array(q1)
    w1 = np.array(q1) - 1.5 * iqr # bottom whisker
    w2 = np.array(q3) + 1.5 * iqr # top whisker
    
    s4 = ColumnDataSource(dict(clust=[i for i in range(len(df[cluster_id].unique()))],q1=q1, q2=q2, q3=q3, w1=w1, w2=w2))
    
    p5 = figure(plot_width=600, plot_height=500, x_range=l_cat[0], tools="pan,wheel_zoom,box_zoom,reset,hover", title="Boxplot for numerical features") 
    hover3 = p5.select(dict(type=HoverTool)) # or p1
    hover3.tooltips={"clust": "@clust","median (Q1, Q3)" : "@q2 (@q1, @q3)"}
    
    #bar = p4.vbar('clust', top='prev', source=s4)
    b1 = p5.vbar('clust', bottom='q2', top='q3', fill_color="#E08E79", line_color="black", source=s4)
    b2 = p5.vbar('clust', bottom='q1', top='q2', fill_color="#3B8686", line_color="black", source=s4)
                          
    # stems
    p5.segment(x0='clust', y0='w2',x1='clust', y1='q3', line_width=2, line_color="black", source=s4)
    p5.segment(x0='clust', y0='w1',x1='clust', y1='q1', line_width=2, line_color="black", source=s4)                      
    
    # whiskers (almost-0 height rects simpler than segments)
    p5.rect('clust', 'w1', 0.2, 0.01, line_color="black", source=s4)
    p5.rect('clust', 'w2', 0.2, 0.01, line_color="black", source=s4)

    color_select = Select(title="color", value=l_cat[0], 
                        options = l_cat,)
    color_select.js_on_change('value', CustomJS(args=dict(cir=cir, s1=s1, s3=s3, s4=s4, cluster_id=cluster_id),
                                          code="""                                                             
        var data = s1.data;
        var bardata = s3.data;
        var violindata = s4.data;
        var gradient = ['#f00', '#fe0500', '#fc0a00', '#fb0f00', '#fa1400', '#f91900', '#f71e00', '#f62200', '#f52700', '#f42c00', '#f23000', '#f13500', '#f03a00', '#ee3e00', '#ed4200', '#ec4700', '#eb4b00', '#e94f00', '#e85400', '#e75800', '#e65c00', '#e46000', '#e36400', '#e26800', '#e16c00', '#df7000', '#de7300', '#d70', '#db7b00', '#da7f00', '#d98200', '#d88600', '#d68900', '#d58d00', '#d49000', '#d39300', '#d19700', '#d09a00', '#cf9d00', '#cda000', '#cca300', '#cba600', '#caa900', '#c8ac00', '#c7af00', '#c6b200', '#c5b500', '#c3b800', '#c2ba00', '#c1bd00', '#bfbf00', '#babe00', '#b5bd00', '#b0bc00', '#acba00', '#a7b900', '#a2b800', '#9db700', '#98b500', '#94b400', '#8fb300', '#8ab200', '#86b000', '#81af00', '#7dae00', '#79ac00', '#74ab00', '#70aa00', '#6ca900', '#68a700', '#64a600', '#60a500', '#5ca400', '#58a200', '#54a100', '#50a000', '#4c9e00', '#489d00', '#459c00', '#419b00', '#3d9900', '#3a9800', '#369700', '#339600', '#2f9400', '#2c9300', '#299200', '#269100', '#228f00', '#1f8e00', '#1c8d00', '#198b00', '#168a00', '#138900', '#108800', '#0d8600', '#0b8500', '#088400', '#058300', '#038100', '#008000'];
        //var categoric = ["#FF0000FF", "#CCFF00FF", "#49BA2B", "#0066FFFF", "#CC00FFFF", '#FF9595'];
        var categoric = ["#1578EC", "#ff9940", "#31D631", "#dd4d4e", "#7a4da4", "#a67c73", "#FF0000FF", "#CCFF00FF", "#49BA2B", "#0066FFFF", "#CC00FFFF", '#FF9595'];
        var grey = '#bcbcbc';
        var cluster_col = cluster_id;

        var selected_color = cb_obj.value;

        console.log(cb_obj.value)
        
        data["desc"] = [] ;
        for (var i=0;i<data["x"].length; i++) {
        data["desc"].push(data[selected_color][i]);
        };

        // Custom max
        function findmax() {
                var par = []
                for (var i = 0; i < arguments.length; i++) {
                    if (!isNaN(arguments[i])) {
                        par.push(arguments[i]);
                    }
                }
                return Math.max.apply(Math, par);
            }

        var max = data[selected_color].reduce(function(a, b) {
            return findmax(a, b);
        });
        

        data["c_col"] = [] ;
        for (var i=0;i<data["x"].length; i++) {
            if (selected_color == cluster_col){
                data["c_col"].push(categoric[data[selected_color][i]]); // maybe + 1
            } else {
                if (isNaN(data[selected_color][i])) {
                    data["c_col"].push(grey);
                } else {
                    var ix = (Math.floor((data[selected_color][i]/max) * 100))
                    data["c_col"].push(gradient[ix]);
                };
                
            };
        };
        
        
        // Calculate prevalence of the feature for the barchart
        var cat = selected_color;
        
        var q1 = [];
        var q2 = [];
        var q3 = [];
        var l_w1 = [];
        var l_w2 = [];
           
        bardata["prev"] = [] // is this necessary?
        var l_prev = [];
        for (var c=0;c<bardata["clust"].length; c++) {
            var score = 0;
            var total = 0;
            var arr = []; // collect actual values
            for (var i=0;i<data["x"].length; i++) {
                if (data["clust"][i] == c ) {
                    if (!isNaN(data[cat][i])) {
                        if (data[cat][i] == 1){
                                score += 1;
                            };
                            arr.push(data[cat][i])
                        };
                        total += 1;
                    };
                };
            // update prevalence in bar chart based on selected feature
            l_prev.push((score/total).toFixed(3));
            
            
            // Calculate IQR
            function quantile(arr, q) {
                var sorted = arr.sort((a, b) => a - b);
                var pos = (sorted.length - 1) * q;
                var base = Math.floor(pos);
                var rest = pos - base;
                if (sorted[base + 1] !== undefined) {
                    return sorted[base] + rest * (sorted[base + 1] - sorted[base]);
                } else {
                    return sorted[base];
                }
            };
            var q25 = quantile(arr, .25);
            var q50 = quantile(arr, .50);
            var q75 = quantile(arr, .75);
            
            var iqr = q75 - q25;
            var w1 = q25 - 1.5 * iqr
            var w2 = q75 + 1.5 * iqr
            
            q1.push(q25);
            q2.push(q50);
            q3.push(q75);
            l_w1.push(w1); 
            l_w2.push(w2); 
            
            };
        bardata["prev"] = l_prev;
        
        
        console.log(q1)
        console.log(l_w1)
        console.log(l_w2)

        // Calculate q1, q2, q3 of the feature for the violinplot
        violindata["q1"] = q1;
        violindata["q2"] = q2;
        violindata["q3"] = q3;
        violindata["w1"] = l_w1;
        violindata["w2"] = l_w2;
            
        
            
        

        cir.glyph.line_color.field = "c_col";
        cir.glyph.fill_color.field = "c_col";

        s1.change.emit() // update 
        s3.change.emit() // update barchart
        s4.change.emit() // update barchart
    """)) # dict[cb_obj.value]

    hover = p3.select(dict(type=HoverTool)) # or p1
    hover.tooltips={"ID": "@pt","value" : "@desc"}
    
    
    def getSummary(col, cluster, cluster_ix):
        if col.name == cluster_id:
            return cluster_ix
        elif col.dtype == float:
            return round(col.mean(), 3)
        elif col.dtype == int and col.max() < 3:
            return round(len(col[col==1])/len(col), 3) #col.value_counts()#/len(col)
        elif col.dtype == int and col.max() > 2:
            return round(col.mean(), 3) # round to 2 decimals
    
    
    new_df = pd.DataFrame() 
    
    # or from sub cluster?
    # by default cluster 0
    new_df[0] = df[df[cluster_id]==cluster_ix][l_cat].apply(lambda x : getSummary(x, cluster_id, cluster_ix))
    new_df = new_df.reset_index()
    new_df.columns = ['var', 'meanprev']
    
    s2 = ColumnDataSource(new_df)

    columns = [
            TableColumn(field="var", title="Variable"),
            TableColumn(field="meanprev", title="Mean or Prevalence"),
        ]
    tb = DataTable(source=s2 , columns=columns, width=400, height=280)


    alp = Slider(start=0, end=1, value=0.1, step=.01, title="Alpha")
    
    
    cluster_select = Select(title="Select cluster", value=str(cluster_ix), 
                        options = [str(i) for i in df[cluster_id].unique()])
    
    cluster_select.js_on_change('value', CustomJS(args=dict(tb=tb, s2=s2, s1=s1, alp=alp, l_lab=l_cat, l_binary=l_binary, clust = cluster_id),
                                          code="""
        var l_lab = l_lab;
        var l_cat = l_binary;
        var clust = clust;
        var data = s2.data;
        var all = s1.data;
        var alpha = alp.value;
        var selected_number = cb_obj.value;

        console.log(l_lab)
        
        data["meanprev"] = [] ;
        for (var j=0;j<l_lab.length; j++){
            var cat = l_lab[j];
            
            var l_rf = [];
            var sum = 0;
            
            for (var i=0;i<all['x'].length; i++) {
                if (all[clust][i] == cb_obj.value ) {
                    if (!isNaN(all[cat][i])) {
                        l_rf.push(all[cat][i]);
                        if (l_cat.includes(cat)) {
                            // if categorical => count prevalence
                            if (all[cat][i] == 1){
                                sum += 1;
                                };
                            } else {
                                // if numerical => calculate mean
                                sum += all[cat][i];
                            };
                        };
                    };
                };
            
            data["meanprev"].push((sum/l_rf.length).toFixed(3));
        };

        data["meanprev"].push(selected_number);
        
        // change alpha ? 
        all["c_alpha"] = [];
        for (var i=0;i<all["x"].length; i++) {
            if (all[clust][i] == cb_obj.value ) {
                all["c_alpha"].push(1);
            } else {
                all["c_alpha"].push(alpha);
            };
        };
        
        
        s1.change.emit()
        s2.change.emit()
    """)) 

    alp.js_on_change('value', CustomJS(args=dict(tb=tb, s1=s1, cs=cluster_select, clust = cluster_id),
                                          code="""
        var all = s1.data;
        var alpha = cb_obj.value;
        var clust = clust;
        
        console.log(cb_obj.value)
        
        // change alpha ? 
        all["c_alpha"] = [];
        for (var i=0;i<all["x"].length; i++) {
            if (all[clust][i] == cs.value ) {
                all["c_alpha"].push(1);
            } else {
                all["c_alpha"].push(alpha);
            };
        };
        
        s1.change.emit()
                                          
                                          """))


    
    

    layout = gridplot([[p3, column(tb, alp, cluster_select)],[color_select, ], [p4, p5]]) # 
    
    if path == None:
        bio.output_file("../TSNE/Baseline_umap_%s.html" % (title), mode='inline')
    else :
        bio.output_file(path, mode='inline')
    bio.show(layout)

    print('\nUMAP figure saved under location: TSNE/Baseline_umap_%s.html' % (title))
    return

#from keras import backend as K
#f.set_session(K.tf.Session(config=K.tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)))

np.random.seed(0)

df_categoric= pd.read_csv('/exports/reum/tdmaarseveen/RA_Clustering/new_data/5_clustering/df_categoric.csv', sep=',')
l_lab = [col for col in df_categoric.columns if col not in ['pseudoId']] 
df_categoric = df_categoric[l_lab]

df_numeric = pd.read_csv('/exports/reum/tdmaarseveen/RA_Clustering/new_data/5_clustering/df_lab_scaled_demographics.csv', sep=',')[['Leuko', 'MCH', 'Hb', 'Ht', 'MCHC',  'MCV', 'Trom', 'BSE',  'Age']] #  'Lym', 'Mono',


# Use pseudo labels

if CREATE_MODEL:

    # Compute the shap values
    t0 = time.time()

    # Set network parameters
    input_shapes = {'Categoric': df_categoric.shape[1:], 'Numeric': df_numeric.shape[1:]} # [df_categoric.shape[1:], (df_numeric.shape[1:])]

    data = [np.array(df_categoric.values), np.array(df_numeric.values)] 

    # Number of units of each layer of encoder network
    hidden_dims = [128, 64, 8] # 8
    # Output activation functions for each modality
    output_activations = ['sigmoid' , 'relu'] # , 'relu'
    # Name of Keras optimizer
    optimizer = 'adam'
    # Loss functions corresponding to a noise model for each modality
    loss = ['bernoulli_divergence', 'gaussian_divergence'] #['bernoulli_divergence']  # ['bernoulli_divergence', 'gaussian_divergence']

    # Construct autoencoder network
    autoencoder = MultimodalAutoencoder(input_shapes, hidden_dims,
                                        output_activations)
    autoencoder.compile(optimizer, loss)

    # Train model where input and output are the same
    history = autoencoder.fit(data,  batch_size=256, epochs=100, validation_split=0.2)  # 200, validation_split=0.15
    # early stopping -> because we get overfitting if we were to further increase the epochs (maybe in the future try to add drop out nodes?)

    # plot learning curves
    pyplot.title('Learning Curves')
    pyplot.xlabel('Epoch')
    pyplot.ylabel('Cross Entropy')
    pyplot.plot(history.history['loss'], label='train')
    pyplot.plot(history.history['val_loss'], label='val')
    pyplot.legend()
    pyplot.savefig('/exports/reum/tdmaarseveen/RA_Clustering/figures/3_clustering/MMAE_convergence_model.png', dpi=100)


    # Get Latent space
    z_filtered = autoencoder.encode(data)

    #z = maui_model.fit_transform({ 'Categorical': df_categoric.T, 'Lab_numerical': df_numeric.T})
    t1 = time.time()
    print('Creating MMAE model: ' + str(t1-t0))

else : 
    data = [np.array(df_categoric.values), np.array(df_numeric.values)]

    autoencoder=tf.keras.models.load_model('/exports/reum/tdmaarseveen/RA_Clustering/models/MMAE', compile=False)
    z_filtered = autoencoder.encoder.predict(data)
    
    
# Writing to file
np.savetxt('/exports/reum/tdmaarseveen/RA_Clustering/figures/3_clustering/mmae_embedding.txt', z_filtered) 

# Write to csv
#df_clustering = pd.DataFrame({'Label_concat' : maui_label})
#df_clustering.to_csv('/exports/reum/tdmaarseveen/RA_Clustering/figures/3_clustering/df_complete_labels_maui.csv', index=False, sep=',')

# Create interactive TSNE with metadata
X_embedded = TSNE(n_components=2, random_state=7062021).fit_transform(z_filtered)

# Open metadata
df_imp = pd.read_csv('/exports/reum/tdmaarseveen/RA_Clustering/new_data/5_clustering/df_metadata.csv')
df_imp['patnr'] = range(len(df_imp))

# Get more insight by visualizing the ratios
df_imp['Swollen ratio'] = df_imp['SJC'] / (df_imp['SJC']+df_imp['TJC'])
df_imp['Swollen ratio'] = df_imp['Swollen ratio'].fillna(0)

df_imp['Tender ratio'] = df_imp['TJC'] / (df_imp['SJC']+df_imp['TJC'])
df_imp['Tender ratio'] = df_imp['Tender ratio'].fillna(0)

df_imp['Big ratio'] = df_imp['Big joints'] / (df_imp['TJC']+df_imp['SJC'])
df_imp['Big ratio'] = df_imp['Big ratio'].fillna(0)

df_imp['Small ratio'] = df_imp['Small joints'] / (df_imp['TJC']+df_imp['SJC'])
df_imp['Small ratio'] = df_imp['Small ratio'].fillna(0)

df_imp['Symmetrical ratio'] = df_imp['Symmetrical joints'] / (df_imp['TJC']+df_imp['SJC'])
df_imp['Symmetrical ratio'] = df_imp['Symmetrical ratio'].fillna(0)

# write coordinates embedding
df_imp['coor_x'] = X_embedded[:, 0]
df_imp['coor_y'] = X_embedded[:, 1]

# Create t-sne with MMAE
df_imp['MMAE_clusters'] = np.random.randint(4, size=len(df_imp['coor_x']))

visualize_umap_bokeh_na(X_embedded, df_imp, list(df_imp.columns), l_binary=l_cat, patient_id='patnr', cluster_id='MMAE_clusters', title='MMAE_UMAP',  path='/exports/reum/tdmaarseveen/RA_Clustering/TSNE/MMAE_all_new3.html')

# Save MMAE embedding
l_embedding = ['patnr', 'coor_x', 'coor_y','MMAE_clusters']
df = pd.DataFrame(z_filtered)
for ix, column in enumerate(df): # z-filtered might be row based
    df_imp['LF_%s' % (ix)] = df[column]
    l_embedding.append('LF_%s' % (ix))
df_imp[l_embedding].to_csv('/exports/reum/tdmaarseveen/RA_Clustering/results/embedding/%s.csv' % EMBEDDING_SPACE, index=False, sep=',')

print('CATEGORIC_FEATURES:', df_categoric.shape[1:])


# Save as Tensorflow JS model 
import tensorflowjs as tfjs

# Save encoder
tfjs.converters.save_keras_model(autoencoder.encoder, "../../models/model_js/mmae_tfjs_encoder")

# Save decoder
tfjs.converters.save_keras_model(autoencoder.decoder, "../../models/model_js/mmae_tfjs_decoder")

print('Finished exporting model')




